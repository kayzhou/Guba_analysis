{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from thulac import thulac\n",
    "thu = thulac(seg_only=True)\n",
    "\n",
    "def load_stopword():\n",
    "    \"\"\"\n",
    "    加载停用词集合\n",
    "    \"\"\"\n",
    "    return set(json.load(open('data/stopword-zh.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取词向量出错：行 1012\n",
      "词向量大小 2000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "def load_label_sentence():\n",
    "    \"\"\"\n",
    "    加载原始文本\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    in_dir = 'data/labelled'\n",
    "\n",
    "    for in_name in os.listdir(in_dir):\n",
    "        _in = os.path.join(in_dir, in_name)\n",
    "        # print(_in)\n",
    "        for i, line in enumerate(open(_in)):\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            label = line.split('\\t')[0]\n",
    "            s= line.split('\\t')[1]\n",
    "            # 1234：四种情绪，-：没有情绪，x：不确定\n",
    "            if label in ['1', '2', '3', '4', '-']:\n",
    "                if label == '-':\n",
    "                    labels.append('0')\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "            sentences.append(s)\n",
    "\n",
    "    return labels, sentences\n",
    "\n",
    "\n",
    "def get_word_freq():\n",
    "    \"\"\"\n",
    "    统计高频词汇\n",
    "    \"\"\"\n",
    "    stopwords = load_stopword()\n",
    "    words_freq = {}\n",
    "    words_ci = {} # 出现某个词，是某类的概率，此问题有五类\n",
    "    class_num = 5\n",
    "    labels_num = [0] * class_num\n",
    "    labels, sentences = load_label_sentence()\n",
    "    \n",
    "    for y, s in zip(labels, sentences):\n",
    "        \n",
    "        # 统计每个类别的数量\n",
    "        labels_num[int(y)] += 1\n",
    "        # 分词\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # 停用词等过滤\n",
    "            if w == '' or w in stopwords or w.isdigit():\n",
    "                continue\n",
    "            elif w in words_freq:\n",
    "                words_freq[w] += 1\n",
    "                words_ci[w][int(y)] += 1\n",
    "            else:\n",
    "                words_freq[w] = 1\n",
    "                words_ci[w] = [0] * class_num\n",
    "                words_ci[w][int(y)] += 1\n",
    "    \n",
    "    # 数量转概率\n",
    "    num2pro = lambda nums: [num / sum(nums) for num in nums]\n",
    "    \n",
    "    # 每类上的概率\n",
    "    v_ci = num2pro(labels_num)\n",
    "    \n",
    "    word_gain = {}\n",
    "    for w in words_ci.keys():\n",
    "        word_ci = words_ci[w]\n",
    "        \n",
    "        v_ci_t = num2pro(word_ci) # 句子出现t是Ci类的概率\n",
    "        \n",
    "        non_word_ci = [labels_num[i] - word_ci[i] for i in range(class_num)] # 不是t时候的各类数量\n",
    "        v_ci_non_t = num2pro(non_word_ci) # 句子不出现t是Ci的概率\n",
    "        \n",
    "        pr_t = words_freq[w] / sum(labels_num) # 存在t的概率\n",
    "        \n",
    "        Gt = Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t)\n",
    "        \n",
    "        word_gain[w] = Gt\n",
    "        \n",
    "\n",
    "    word_gain = sorted(word_gain.items(), key=lambda d: d[1], reverse=True) \n",
    "    with open('data/word_gain_freq.txt', 'w') as f:\n",
    "        for w, gain in word_gain:\n",
    "            if words_freq[w] >= 5:\n",
    "                print(w, gain, words_freq[w], sep='\\t', file=f)\n",
    "            \n",
    "\n",
    "            \n",
    "def Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t):\n",
    "    \"\"\"\n",
    "    计算信息增益，需要每类的概率，句子出现t是Ci类的概率，不出现t是Ci的概率，存在t的概率\n",
    "    \"\"\"\n",
    "    def info_entropy(p):\n",
    "        if p == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -p * np.log(p)\n",
    "    \n",
    "    gain = 0\n",
    "    for i in range(len(v_ci)):\n",
    "        gain = gain + (info_entropy(v_ci[i]) - pr_t * info_entropy(v_ci_t[i]) - (1 - pr_t) * info_entropy(v_ci_non_t[i]))\n",
    "    return gain\n",
    "    \n",
    "\n",
    "def word_2_vec_one_hot():\n",
    "\n",
    "    def load_word_list(first=2000):\n",
    "        word_list = []\n",
    "        for i, line in enumerate(open('data/word_gain_freq.txt')):\n",
    "            if i >= first:\n",
    "                break\n",
    "            try:\n",
    "                w, gain, freq = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print('读取词向量出错：行 {}'.format(i))\n",
    "            word_list.append(w)\n",
    "        print('词向量大小', len(word_list))\n",
    "        return word_list\n",
    "\n",
    "    word_list = load_word_list()\n",
    "    labels, sentences = load_label_sentence()\n",
    "    i = 0\n",
    "    for y, s in zip(labels, sentences):\n",
    "        i += 1\n",
    "        if not i % 1000:\n",
    "            print(i)\n",
    "        vec = np.zeros(len(word_list))\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # print(w)\n",
    "            try:\n",
    "                _i = word_list.index(w)\n",
    "                vec[_i] = 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        print(y, ','.join(['{:.1f}'.format(num) for num in list(vec)]), sep='\\t', file=open('train_data_one_hot-20180710.txt', 'a'))\n",
    "    \n",
    "# one-hot \n",
    "get_word_freq() # 词分析\n",
    "word_2_vec_one_hot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载词向量中 ...\n",
      "加载词完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kay/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "def load_word_vec():\n",
    "    \"\"\"\n",
    "    加载ACL2018词向量\n",
    "    \"\"\"\n",
    "    word_vec = {}\n",
    "    print('加载词向量中 ...')\n",
    "    for i, line in enumerate(open('data/sgns.merge.word')):\n",
    "#         if i <= 100:\n",
    "#             continue\n",
    "        if i > 10000:\n",
    "            break\n",
    "        words = line.strip().split(' ')\n",
    "        word = words[0]\n",
    "        vec = np.array([float(num) for num in words[1:]])\n",
    "        word_vec[word] = vec\n",
    "    print('加载词完成！')\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "# 建立训练文件\n",
    "word_vec = load_word_vec()\n",
    "labels, sentences = load_label_sentence()\n",
    "i = 0\n",
    "for y, s in zip(labels, sentences):\n",
    "    i += 1\n",
    "    if not i % 1000:\n",
    "        print(i)\n",
    "    vec = np.zeros(300)\n",
    "    count = 0\n",
    "    for w in thu.cut(s):\n",
    "        w = w[0]\n",
    "        if w[0] in word_vec:\n",
    "            vec += word_vec[w[0]]\n",
    "            count += 1\n",
    "    vec = vec / count\n",
    "    if count > 0:\n",
    "        print(y, ','.join(['{:.6f}'.format(num) for num in list(vec)]), sep='\\t', file=open('train_data_ACL-20180710.txt', 'a'))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~ ⬆️准备训练数据 ⬇️开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(in_name):\n",
    "    \"\"\"\n",
    "    加载训练数据\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for line in open(in_name):\n",
    "        label, vec = line.strip().split('\\t')\n",
    "        x = np.array([float(v) for v in vec.split(',')])\n",
    "        y.append(label)\n",
    "        X.append(x)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train():\n",
    "#     X, y = load_train_data('train_data_one_hot-20180710.txt')\n",
    "    X, y = load_train_data('train_data_ACL-20180710.txt')\n",
    "    \n",
    "    # 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "    \n",
    "    # 初始化分类器\n",
    "#     clf = RandomForestClassifier(max_depth=10, random_state=1)\n",
    "#     clf = BernoulliNB()\n",
    "    clf = SVC(C=0.5) # SVM较为耗时\n",
    "    \n",
    "    # 执行训练\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # 模型评估\n",
    "    print(cross_val_score(clf, X, y, cv=10).mean())\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        y = clf.predict(X_test[i].reshape(1, -1))\n",
    "        # print(y[0])\n",
    "        y_pred.append(y[0])\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "# train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
